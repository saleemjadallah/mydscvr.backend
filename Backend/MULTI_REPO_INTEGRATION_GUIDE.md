# Multi-Repository Integration Guide

## üèóÔ∏è Current Architecture

Your system has two separate repositories with CI/CD:

```
üìÅ mydscvr.backend
‚îú‚îÄ‚îÄ utils/                    # ‚Üê Shared utilities
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ routers/
‚îî‚îÄ‚îÄ Backend API code

üìÅ mydscvr.datacollection  
‚îú‚îÄ‚îÄ enhanced_collection.py   # ‚Üê Needs utils/
‚îú‚îÄ‚îÄ run_collection_with_ai.sh
‚îî‚îÄ‚îÄ Data collection code
```

**Problem**: Datacollection needs `utils/` from backend, but they're separate repos.

## üéØ Solution Options (Ranked by Recommendation)

### **Option 1: GitHub Actions Auto-Sync (RECOMMENDED)**

**How it works**: GitHub Actions automatically syncs `utils/` from backend to datacollection

**Pros**:
- ‚úÖ Fully automated
- ‚úÖ Triggered by backend changes
- ‚úÖ Maintains repo separation
- ‚úÖ Works with existing CI/CD

**Setup**:
1. Add `github_actions_utils_sync.yml` to `mydscvr.datacollection/.github/workflows/`
2. Configure GitHub secrets if backend is private
3. Backend changes automatically update datacollection

**Example Flow**:
```
Backend updated ‚Üí GitHub Action ‚Üí Sync utils ‚Üí Commit to datacollection ‚Üí Deploy
```

---

### **Option 2: Server-Side Sync Script**

**How it works**: Script on server syncs utils between deployed repos

**Pros**:
- ‚úÖ Simple to implement
- ‚úÖ Works with current setup
- ‚úÖ Can run on schedule

**Setup**:
```bash
# Copy sync script to server
scp Backend/sync_utils_to_datacollection.sh ubuntu@server:/home/ubuntu/

# Add to cron (runs before each collection)
59 23 * * * /home/ubuntu/sync_utils_to_datacollection.sh >> /home/ubuntu/logs/utils_sync.log 2>&1
0 0 * * * cd /home/ubuntu/mydscvr-datacollection && ./run_collection_with_ai.sh
```

---

### **Option 3: Git Submodules**

**How it works**: Make backend a submodule of datacollection

**Pros**:
- ‚úÖ Version-controlled dependencies
- ‚úÖ Git-native solution
- ‚úÖ Clean architecture

**Cons**:
- ‚ùå More complex git operations
- ‚ùå Requires learning submodules

**Setup**:
```bash
# In datacollection repo
git submodule add https://github.com/saleemjadallah/mydscvr.backend.git backend
# Access utils via: backend/utils/
```

---

### **Option 4: Python Package (Future-Proof)**

**How it works**: Publish utils as Python package

**Pros**:
- ‚úÖ Industry standard
- ‚úÖ Version management
- ‚úÖ Clean imports

**Cons**:
- ‚ùå More initial setup
- ‚ùå Need package registry

**Setup**:
```python
# In backend repo, create setup.py
# Publish to PyPI or private registry
# In datacollection: pip install mydscvr-utils==1.0.0
```

## üöÄ Recommended Implementation: Option 1 + Option 2

**Hybrid Approach** (Best of both worlds):

1. **GitHub Actions** for development and CI/CD automation
2. **Server sync script** as backup/manual override

### **Step 1: GitHub Actions Setup**

```bash
# 1. Add workflow to datacollection repo
cp Backend/github_actions_utils_sync.yml /path/to/mydscvr.datacollection/.github/workflows/

# 2. Push to enable automation
cd /path/to/mydscvr.datacollection
git add .github/workflows/github_actions_utils_sync.yml
git commit -m "Add automated utils sync from backend"
git push
```

### **Step 2: Server Backup Script**

```bash
# Copy sync script to server
scp Backend/sync_utils_to_datacollection.sh ubuntu@3.29.102.4:/home/ubuntu/mydscvr-datacollection/
ssh ubuntu@3.29.102.4 "chmod +x /home/ubuntu/mydscvr-datacollection/sync_utils_to_datacollection.sh"

# Test the sync
ssh ubuntu@3.29.102.4 "cd /home/ubuntu/mydscvr-datacollection && ./sync_utils_to_datacollection.sh"
```

### **Step 3: Update Cron Job**

```bash
# Enhanced cron job with utils sync
0 0 * * * cd /home/ubuntu/mydscvr-datacollection && ./sync_utils_to_datacollection.sh >> logs/utils_sync.log 2>&1 && ./run_collection_with_ai.sh >> logs/cron_output.log 2>&1
```

## üìä Comparison Matrix

| Solution | Setup Complexity | Automation | Maintenance | Reliability |
|----------|------------------|------------|-------------|-------------|
| GitHub Actions | Medium | High | Low | High |
| Server Sync | Low | Medium | Medium | Medium |
| Git Submodules | High | High | Low | High |
| Python Package | High | High | Low | Very High |

## üîÑ Workflow Examples

### **Development Workflow** (GitHub Actions):
```
1. Update utils in backend repo ‚Üí Push
2. GitHub Action automatically syncs to datacollection
3. Datacollection auto-deploys with latest utils
4. Cron job runs with current utils
```

### **Emergency Workflow** (Server Sync):
```
1. SSH to server
2. Run manual sync: ./sync_utils_to_datacollection.sh
3. Verify: ./run_collection_with_ai.sh
```

## üõ†Ô∏è Current Status & Next Steps

### **Immediate Actions** (Choose One):

**For GitHub Actions approach**:
```bash
# Add workflow file to datacollection repo
# Enable automatic syncing
```

**For Server Sync approach**:
```bash
# Deploy sync script to server
# Update cron job to include sync
```

### **Long-term Strategy**:
1. **Short-term**: Use server sync for immediate reliability
2. **Medium-term**: Implement GitHub Actions for automation  
3. **Long-term**: Consider Python package for scalability

## ‚úÖ Success Criteria

Your integration is successful when:
- ‚úÖ Datacollection always has latest utils
- ‚úÖ Cron jobs never fail due to missing dependencies
- ‚úÖ Backend updates automatically propagate
- ‚úÖ Manual override available for emergencies
- ‚úÖ Clear logs for troubleshooting

**Current Priority**: Fix the immediate cron job dependency issue, then implement automated syncing for future reliability. 